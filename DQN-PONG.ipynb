{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728be98b",
   "metadata": {},
   "source": [
    "HELPERWRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714ebd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from lz4.block import compress, decompress\n",
    "\n",
    "\n",
    "\n",
    "class LazyFrames:\n",
    "    \"\"\"Ensures common frames are only stored once to optimize memory use.\n",
    "\n",
    "    To further reduce the memory use, it is optionally to turn on lz4 to compress the observations.\n",
    "\n",
    "    Note:\n",
    "        This object should only be converted to numpy array just before forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\"frame_shape\", \"dtype\", \"shape\", \"lz4_compress\", \"_frames\")\n",
    "\n",
    "    def __init__(self, frames: list, lz4_compress: bool = False):\n",
    "        \"\"\"Lazyframe for a set of frames and if to apply lz4.\n",
    "\n",
    "        Args:\n",
    "            frames (list): The frames to convert to lazy frames\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "\n",
    "        Raises:\n",
    "            DependencyNotInstalled: lz4 is not installed\n",
    "        \"\"\"\n",
    "        self.frame_shape = tuple(frames[0].shape)\n",
    "        self.shape = (len(frames),) + self.frame_shape\n",
    "        self.dtype = frames[0].dtype\n",
    "        if lz4_compress:\n",
    "            try:\n",
    "                from lz4.block import compress\n",
    "            except ImportError as e:\n",
    "                raise DependencyNotInstalled(\n",
    "                    \"lz4 is not installed, run `pip install gymnasium[other]`\"\n",
    "                ) from e\n",
    "\n",
    "            frames = [compress(frame) for frame in frames]\n",
    "        self._frames = frames\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        \"\"\"Gets a numpy array of stacked frames with specific dtype.\n",
    "\n",
    "        Args:\n",
    "            dtype: The dtype of the stacked frames\n",
    "\n",
    "        Returns:\n",
    "            The array of stacked frames with dtype\n",
    "        \"\"\"\n",
    "        arr = self[:]\n",
    "        if dtype is not None:\n",
    "            return arr.astype(dtype)\n",
    "        return arr\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of frame stacks.\n",
    "\n",
    "        Returns:\n",
    "            The number of frame stacks\n",
    "        \"\"\"\n",
    "        return self.shape[0]\n",
    "\n",
    "    def __getitem__(self, int_or_slice: Union[int, slice]):\n",
    "        \"\"\"Gets the stacked frames for a particular index or slice.\n",
    "\n",
    "        Args:\n",
    "            int_or_slice: Index or slice to get items for\n",
    "\n",
    "        Returns:\n",
    "            np.stacked frames for the int or slice\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(int_or_slice, int):\n",
    "            return self._check_decompress(self._frames[int_or_slice])  # single frame\n",
    "        return np.stack(\n",
    "            [self._check_decompress(f) for f in self._frames[int_or_slice]], axis=0\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Checks that the current frames are equal to the other object.\"\"\"\n",
    "        return self.__array__() == other\n",
    "\n",
    "    def _check_decompress(self, frame):\n",
    "        if self.lz4_compress:\n",
    "            from lz4.block import decompress\n",
    "\n",
    "            return np.frombuffer(decompress(frame), dtype=self.dtype).reshape(\n",
    "                self.frame_shape\n",
    "            )\n",
    "        return frame\n",
    "\n",
    "\n",
    "class FrameStack(gym.ObservationWrapper, gym.utils.RecordConstructorArgs):\n",
    "    \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "    For example, if the number of stacks is 4, then the returned observation contains\n",
    "    the most recent 4 observations. For environment 'Pendulum-v1', the original observation\n",
    "    is an array with shape [3], so if we stack 4 observations, the processed observation\n",
    "    has shape [4, 3].\n",
    "\n",
    "    Note:\n",
    "        - To be memory efficient, the stacked observations are wrapped by :class:`LazyFrame`.\n",
    "        - The observation space must be :class:`Box` type. If one uses :class:`Dict`\n",
    "          as observation space, it should apply :class:`FlattenObservation` wrapper first.\n",
    "        - After :meth:`reset` is called, the frame buffer will be filled with the initial observation.\n",
    "          I.e. the observation returned by :meth:`reset` will consist of `num_stack` many identical frames.\n",
    "\n",
    "    Example:\n",
    "        >>> import gymnasium as gym\n",
    "        >>> from gymnasium.wrappers import FrameStack\n",
    "        >>> env = gym.make(\"CarRacing-v2\")\n",
    "        >>> env = FrameStack(env, 4)\n",
    "        >>> env.observation_space\n",
    "        Box(0, 255, (4, 96, 96, 3), uint8)\n",
    "        >>> obs, _ = env.reset()\n",
    "        >>> obs.shape\n",
    "        (4, 96, 96, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        num_stack: int,\n",
    "        lz4_compress: bool = False,\n",
    "    ):\n",
    "        \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "        Args:\n",
    "            env (Env): The environment to apply the wrapper\n",
    "            num_stack (int): The number of frames to stack\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "        \"\"\"\n",
    "        gym.utils.RecordConstructorArgs.__init__(\n",
    "            self, num_stack=num_stack, lz4_compress=lz4_compress\n",
    "        )\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.num_stack = num_stack\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], num_stack, axis=0)\n",
    "        high = np.repeat(\n",
    "            self.observation_space.high[np.newaxis, ...], num_stack, axis=0\n",
    "        )\n",
    "        self.observation_space = Box(\n",
    "            low=low, high=high, dtype=self.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"Converts the wrappers current frames to lazy frames.\n",
    "\n",
    "        Args:\n",
    "            observation: Ignored\n",
    "\n",
    "        Returns:\n",
    "            :class:`LazyFrames` object for the wrapper's frame buffer,  :attr:`self.frames`\n",
    "        \"\"\"\n",
    "        assert len(self.frames) == self.num_stack, (len(self.frames), self.num_stack)\n",
    "        return LazyFrames(list(self.frames), self.lz4_compress)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Steps through the environment, appending the observation to the frame buffer.\n",
    "\n",
    "        Args:\n",
    "            action: The action to step through the environment with\n",
    "\n",
    "        Returns:\n",
    "            Stacked observations, reward, terminated, truncated, and information from the environment\n",
    "        \"\"\"\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self.observation(None), reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset the environment with kwargs.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: The kwargs for the environment reset\n",
    "\n",
    "        Returns:\n",
    "            The stacked observations\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        [self.frames.append(obs) for _ in range(self.num_stack)]\n",
    "\n",
    "        return self.observation(None), info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca77ccc",
   "metadata": {},
   "source": [
    "BUFFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65cdb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, max_size, input_shape, device='cpu'):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_ctr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.uint8)\n",
    "        self.next_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.uint8)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        if self.mem_ctr > (batch_size * 5):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        index = self.mem_ctr % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.action_memory[index] = torch.tensor(action).detach().cpu()\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_ctr += 1\n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_ctr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b27ab",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06265025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, action_dim, hidden_dim=512, observation_shape=(4, 84, 84)):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        # Original: in_channels=1, out_channels=8, kernel_size=4, stride=2\n",
    "        self.conv1 = nn.Conv2d(in_channels=observation_shape[0], out_channels=32, kernel_size=8, stride=4)\n",
    "        # Original: in_channels=8, out_channels=16, kernel_size=4, stride=2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        # Original: in_channels=16, out_channels=32, kernel_size=3, stride=2\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Compute conv output size automatically\n",
    "        conv_output_size = self.calculate_conv_output(observation_shape)\n",
    "        print(\"conv_output_size: \", conv_output_size)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Original had 3 FC layers; now only 2 FC layers before output\n",
    "        self.fc1 = nn.Linear(conv_output_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)  # Optional: can remove this if you want exactly 2 FC\n",
    "\n",
    "        self.output = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.weights_init)\n",
    "\n",
    "    def calculate_conv_output(self, shape):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        o = F.relu(self.conv1(o))\n",
    "        o = F.relu(self.conv2(o))\n",
    "        o = F.relu(self.conv3(o))\n",
    "        return int(o.numel())\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))  # Optional if keeping 3 FC layers\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "    def save_the_model(self, filename='models/latest.pt'):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load_the_model(self, filename='models/latest.pt'):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "            print(f\"Loaded weights from {filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No weights file found at at {filename}\")\n",
    "\n",
    "\n",
    "def soft_update(target, source, tau=0.005):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "# CNN - Recognize Image\n",
    "# FC layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e628a2",
   "metadata": {},
   "source": [
    "AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff77f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import ReplayBuffer\n",
    "from model import Model, soft_update\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, env, hidden_layer, learning_rate, step_repeat, gamma):\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "        self.step_repeat = step_repeat\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        obs, info = self.env.reset()\n",
    "\n",
    "        obs = self.process_observation(obs)\n",
    "\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        print(f'Loaded model on device {self.device}')\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size=500000, input_shape=obs.shape, device=self.device)\n",
    "\n",
    "        self.model = Model(action_dim=env.action_space.n, hidden_dim=hidden_layer, observation_shape=obs.shape).to(self.device)\n",
    "        \n",
    "        self.target_model = Model(action_dim=env.action_space.n, hidden_dim=hidden_layer, observation_shape=obs.shape).to(self.device)\n",
    "\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def process_observation(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1)\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "\n",
    "        self.model.load_the_model()\n",
    "\n",
    "        obs, info = self.env.reset()\n",
    "\n",
    "        done = False\n",
    "        obs, info = self.env.reset()\n",
    "        obs = self.process_observation(obs)\n",
    "\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            if random.random() < 0.05:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                q_values = self.model.forward(obs.unsqueeze(0).to(self.device))[0]\n",
    "                action = torch.argmax(q_values, dim=-1).item()\n",
    "            \n",
    "            reward = 0\n",
    "\n",
    "            for i in range(self.step_repeat):\n",
    "                reward_temp = 0\n",
    "\n",
    "                next_obs, reward_temp, done, truncated, info = self.env.step(action=action)\n",
    "\n",
    "                reward += reward_temp\n",
    "\n",
    "                # frame = self.env.env.env.render() \n",
    "\n",
    "                # resized_frame = cv2.resize(frame, (500, 400))\n",
    "\n",
    "                # resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # cv2.imshow(\"Pong AI\", resized_frame)\n",
    "\n",
    "                # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                #     break\n",
    "\n",
    "                # # time.sleep(0.05) #adjust speed to control playback speed\n",
    "\n",
    "                if(done):\n",
    "                    break\n",
    "            \n",
    "            obs = self.process_observation(next_obs)\n",
    "\n",
    "            episode_reward += reward\n",
    "    \n",
    "\n",
    "    def train(self, episodes, max_episode_steps, summary_writer_suffix, batch_size, epsilon, epsilon_decay, min_epsilon):\n",
    "\n",
    "            # Save TensorBoard logs in the same directory as the script\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            summary_writer_name = os.path.join(\n",
    "                script_dir, 'runs',\n",
    "                f'{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_{summary_writer_suffix}'\n",
    "            )\n",
    "            writer = SummaryWriter(summary_writer_name)\n",
    "\n",
    "            if not os.path.exists('models'):\n",
    "                os.makedirs('models')\n",
    "\n",
    "            total_steps = 0\n",
    "            reward_history, loss_history, qvalue_history = [], [], []\n",
    "\n",
    "            print(f\"TensorBoard logs → {summary_writer_name}\")\n",
    "\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                obs, info = self.env.reset()\n",
    "                obs = self.process_observation(obs)\n",
    "                episode_steps = 0\n",
    "                episode_losses, episode_qvalues, td_errors, action_counts = [], [], [], []\n",
    "\n",
    "                episode_start_time = time.time()\n",
    "\n",
    "                while not done and episode_steps < max_episode_steps:\n",
    "                    # ε-greedy action selection\n",
    "                    if random.random() < epsilon:\n",
    "                        action = self.env.action_space.sample()\n",
    "                    else:\n",
    "                        q_values = self.model.forward(obs.unsqueeze(0).to(self.device))[0]\n",
    "                        action = torch.argmax(q_values, dim=-1).item()\n",
    "                        episode_qvalues.append(q_values.mean().item())\n",
    "\n",
    "                    action_counts.append(action)\n",
    "\n",
    "                    reward = 0\n",
    "                    for _ in range(self.step_repeat):\n",
    "                        next_obs, reward_temp, done, truncated, info = self.env.step(action=action)\n",
    "                        reward += reward_temp\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "                    next_obs = self.process_observation(next_obs)\n",
    "                    self.memory.store_transition(obs, action, reward, next_obs, done)\n",
    "                    obs = next_obs\n",
    "\n",
    "                    episode_reward += reward\n",
    "                    episode_steps += 1\n",
    "                    total_steps += 1\n",
    "\n",
    "                    # Training updates\n",
    "                    if self.memory.can_sample(batch_size):\n",
    "                        observations, actions, rewards, next_observations, dones = self.memory.sample_buffer(batch_size)\n",
    "                        dones = dones.unsqueeze(1).float()\n",
    "\n",
    "                        # Q(s,a)\n",
    "                        q_values = self.model(observations)\n",
    "                        actions = actions.unsqueeze(1).long()\n",
    "                        qsa_batch = q_values.gather(1, actions)\n",
    "\n",
    "                        # Q-target(s’, a’)\n",
    "                        next_actions = torch.argmax(self.model(next_observations), dim=1, keepdim=True)\n",
    "                        next_q_values = self.target_model(next_observations).gather(1, next_actions)\n",
    "                        target_b = rewards.unsqueeze(1) + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "                        # TD error & loss\n",
    "                        td_error = (target_b.detach() - qsa_batch)\n",
    "                        loss = F.mse_loss(qsa_batch, target_b.detach())\n",
    "\n",
    "                        episode_losses.append(loss.item())\n",
    "                        td_errors.extend(td_error.abs().detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "\n",
    "                        # Optimization step\n",
    "                        self.model.zero_grad()\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                        if episode_steps % 4 == 0:\n",
    "                            soft_update(self.target_model, self.model)\n",
    "\n",
    "                        # Log per-step metrics\n",
    "                        writer.add_scalar(\"Loss/step\", loss.item(), total_steps)\n",
    "                        writer.add_scalar(\"QValue/mean\", q_values.mean().item(), total_steps)\n",
    "                        writer.add_scalar(\"QValue/max\", q_values.max().item(), total_steps)\n",
    "                        writer.add_scalar(\"TD_Error/mean\", td_error.abs().mean().item(), total_steps)\n",
    "\n",
    "                # Model checkpoint\n",
    "                self.model.save_the_model()\n",
    "\n",
    "                # Episode metrics\n",
    "                episode_time = time.time() - episode_start_time\n",
    "                avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
    "                avg_qvalue = np.mean(episode_qvalues) if episode_qvalues else 0\n",
    "                avg_td_error = np.mean(td_errors) if td_errors else 0\n",
    "                fps = episode_steps / episode_time if episode_time > 0 else 0\n",
    "                avg_reward = np.mean(reward_history[-10:]) if reward_history else 0\n",
    "\n",
    "                reward_history.append(episode_reward)\n",
    "                loss_history.append(avg_loss)\n",
    "                qvalue_history.append(avg_qvalue)\n",
    "\n",
    "                # TensorBoard logs (episode level)\n",
    "                writer.add_scalar('Score/Episode', episode_reward, episode)\n",
    "                writer.add_scalar('Score/Avg10', avg_reward, episode)\n",
    "                writer.add_scalar('Loss/Episode', avg_loss, episode)\n",
    "                writer.add_scalar('QValue/Avg', avg_qvalue, episode)\n",
    "                writer.add_scalar('TD_Error/EpisodeMean', avg_td_error, episode)\n",
    "                writer.add_scalar('Epsilon', epsilon, episode)\n",
    "                writer.add_scalar('Performance/FPS', fps, episode)\n",
    "                writer.add_scalar('Performance/EpisodeTime', episode_time, episode)\n",
    "\n",
    "                # Action distribution histogram\n",
    "                writer.add_histogram('Action/Distribution', np.array(action_counts), episode)\n",
    "\n",
    "                # Console output\n",
    "                print(f\"\\nEpisode {episode + 1}/{episodes}\")\n",
    "                print(f\"  Reward: {episode_reward:.2f}  |  Avg(10): {avg_reward:.2f}\")\n",
    "                print(f\"  Avg Loss: {avg_loss:.6f}  |  Avg Q: {avg_qvalue:.3f}  |  TD Error: {avg_td_error:.4f}\")\n",
    "                print(f\"  Steps: {episode_steps}  |  FPS: {fps:.2f}  |  Time: {episode_time:.2f}s\")\n",
    "                print(f\"  Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "                # Epsilon decay\n",
    "                if epsilon > min_epsilon:\n",
    "                    epsilon *= epsilon_decay\n",
    "\n",
    "            writer.close()\n",
    "            print(\"\\n✅ Training complete! You can now view metrics with:\")\n",
    "            print(f\"   tensorboard --logdir {os.path.join(script_dir, 'runs')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea1a4f",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0548246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model on device cuda:0\n",
      "conv_output_size:  1024\n",
      "conv_output_size:  1024\n",
      "TensorBoard logs → c:\\Users\\Vernon\\Desktop\\CodingWorkSpace\\project_reinforcementLearning\\runs\\2025-10-22_21-03-19_dqn_lr=0.0001_hl=128_bs=32\n",
      "\n",
      "Episode 1/2500\n",
      "  Reward: -21.00  |  Avg(10): 0.00\n",
      "  Avg Loss: 1732.202429  |  Avg Q: 0.000  |  TD Error: 33.6240\n",
      "  Steps: 218  |  FPS: 128.06  |  Time: 1.70s\n",
      "  Epsilon: 1.0000\n",
      "\n",
      "Episode 2/2500\n",
      "  Reward: -20.00  |  Avg(10): -21.00\n",
      "  Avg Loss: 766.380110  |  Avg Q: -36.276  |  TD Error: 21.0269\n",
      "  Steps: 231  |  FPS: 106.20  |  Time: 2.18s\n",
      "  Epsilon: 0.9950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m\n\u001b[0;32m     30\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(env, hidden_layer\u001b[38;5;241m=\u001b[39mhidden_layer,\n\u001b[0;32m     31\u001b[0m               learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, step_repeat\u001b[38;5;241m=\u001b[39mstep_repeat,\n\u001b[0;32m     32\u001b[0m               gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[0;32m     34\u001b[0m summary_writer_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn_lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hl=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 36\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_episode_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_episode_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43msummary_writer_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_writer_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_epsilon\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\Desktop\\CodingWorkSpace\\project_reinforcementLearning\\agent.py:180\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, episodes, max_episode_steps, summary_writer_suffix, batch_size, epsilon, epsilon_decay, min_epsilon)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    179\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m     soft_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\optim\\optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    514\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m             )\n\u001b[1;32m--> 517\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\optim\\optimizer.py:82\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 82\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\optim\\adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    238\u001b[0m         group,\n\u001b[0;32m    239\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m         state_steps,\n\u001b[0;32m    245\u001b[0m     )\n\u001b[1;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\optim\\optimizer.py:150\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\optim\\adam.py:953\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 953\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\optim\\adam.py:780\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m    777\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m    779\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m--> 780\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(\n\u001b[0;32m    782\u001b[0m     device_params,\n\u001b[0;32m    783\u001b[0m     device_exp_avgs,\n\u001b[0;32m    784\u001b[0m     exp_avg_sq_sqrt,\n\u001b[0;32m    785\u001b[0m     step_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    786\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformObservation, ResizeObservation, GrayscaleObservation\n",
    "\n",
    "import ale_py\n",
    "import cv2\n",
    "\n",
    "def to_grayscale(obs):\n",
    "    return cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "episodes = 2500\n",
    "max_episode_steps = 5000\n",
    "hidden_layer = 128\n",
    "learning_rate = 0.0001\n",
    "step_repeat = 4\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "env = ResizeObservation(env, (64, 64))\n",
    "\n",
    "env = GrayscaleObservation(env,  keep_dim=True)\n",
    "\n",
    "agent = Agent(env, hidden_layer=hidden_layer,\n",
    "              learning_rate=learning_rate, step_repeat=step_repeat,\n",
    "              gamma=gamma)\n",
    "\n",
    "summary_writer_suffix = f'dqn_lr={learning_rate}_hl={hidden_layer}_bs={batch_size}'\n",
    "\n",
    "agent.train(episodes=episodes,\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            summary_writer_suffix=summary_writer_suffix,\n",
    "            batch_size=batch_size,\n",
    "            epsilon=epsilon,\n",
    "            epsilon_decay=epsilon_decay,\n",
    "            min_epsilon=min_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa50a1",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd09263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agent import Agent\n",
    "# import gymnasium as gym\n",
    "# from gymnasium.wrappers import GrayscaleObservation, ResizeObservation\n",
    "# from gymnasium.wrappers import TransformObservation, ResizeObservation\n",
    "\n",
    "# import ale_py\n",
    "# import cv2  \n",
    "\n",
    "# def to_grayscale(obs):\n",
    "#     return cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "# episodes = 10000\n",
    "# max_episode_steps = 10000\n",
    "# hidden_layer = 128\n",
    "# learning_rate = 0.0001\n",
    "# step_repeat = 4\n",
    "# gamma = 0.99\n",
    "# batch_size = 64\n",
    "# epsilon = 1\n",
    "# min_epsilon = 0.1\n",
    "# epsilon_decay = 0.995\n",
    "\n",
    "\n",
    "# env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "# env = ResizeObservation(env, (64, 64))\n",
    "\n",
    "# env = GrayscaleObservation(env,  keep_dim=True)\n",
    "\n",
    "# # env = TransformObservation(env,  f=to_grayscale)\n",
    "\n",
    "# agent = Agent(env, hidden_layer=hidden_layer,\n",
    "#               learning_rate=learning_rate, step_repeat=step_repeat,\n",
    "#               gamma=gamma)\n",
    "\n",
    "# agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
