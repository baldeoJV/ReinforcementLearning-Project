{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728be98b",
   "metadata": {},
   "source": [
    "HELPERWRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "714ebd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from lz4.block import compress, decompress\n",
    "\n",
    "\n",
    "\n",
    "class LazyFrames:\n",
    "    \"\"\"Ensures common frames are only stored once to optimize memory use.\n",
    "\n",
    "    To further reduce the memory use, it is optionally to turn on lz4 to compress the observations.\n",
    "\n",
    "    Note:\n",
    "        This object should only be converted to numpy array just before forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\"frame_shape\", \"dtype\", \"shape\", \"lz4_compress\", \"_frames\")\n",
    "\n",
    "    def __init__(self, frames: list, lz4_compress: bool = False):\n",
    "        \"\"\"Lazyframe for a set of frames and if to apply lz4.\n",
    "\n",
    "        Args:\n",
    "            frames (list): The frames to convert to lazy frames\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "\n",
    "        Raises:\n",
    "            DependencyNotInstalled: lz4 is not installed\n",
    "        \"\"\"\n",
    "        self.frame_shape = tuple(frames[0].shape)\n",
    "        self.shape = (len(frames),) + self.frame_shape\n",
    "        self.dtype = frames[0].dtype\n",
    "        if lz4_compress:\n",
    "            try:\n",
    "                from lz4.block import compress\n",
    "            except ImportError as e:\n",
    "                raise DependencyNotInstalled(\n",
    "                    \"lz4 is not installed, run `pip install gymnasium[other]`\"\n",
    "                ) from e\n",
    "\n",
    "            frames = [compress(frame) for frame in frames]\n",
    "        self._frames = frames\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        \"\"\"Gets a numpy array of stacked frames with specific dtype.\n",
    "\n",
    "        Args:\n",
    "            dtype: The dtype of the stacked frames\n",
    "\n",
    "        Returns:\n",
    "            The array of stacked frames with dtype\n",
    "        \"\"\"\n",
    "        arr = self[:]\n",
    "        if dtype is not None:\n",
    "            return arr.astype(dtype)\n",
    "        return arr\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of frame stacks.\n",
    "\n",
    "        Returns:\n",
    "            The number of frame stacks\n",
    "        \"\"\"\n",
    "        return self.shape[0]\n",
    "\n",
    "    def __getitem__(self, int_or_slice: Union[int, slice]):\n",
    "        \"\"\"Gets the stacked frames for a particular index or slice.\n",
    "\n",
    "        Args:\n",
    "            int_or_slice: Index or slice to get items for\n",
    "\n",
    "        Returns:\n",
    "            np.stacked frames for the int or slice\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(int_or_slice, int):\n",
    "            return self._check_decompress(self._frames[int_or_slice])  # single frame\n",
    "        return np.stack(\n",
    "            [self._check_decompress(f) for f in self._frames[int_or_slice]], axis=0\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Checks that the current frames are equal to the other object.\"\"\"\n",
    "        return self.__array__() == other\n",
    "\n",
    "    def _check_decompress(self, frame):\n",
    "        if self.lz4_compress:\n",
    "            from lz4.block import decompress\n",
    "\n",
    "            return np.frombuffer(decompress(frame), dtype=self.dtype).reshape(\n",
    "                self.frame_shape\n",
    "            )\n",
    "        return frame\n",
    "\n",
    "\n",
    "class FrameStack(gym.ObservationWrapper, gym.utils.RecordConstructorArgs):\n",
    "    \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "    For example, if the number of stacks is 4, then the returned observation contains\n",
    "    the most recent 4 observations. For environment 'Pendulum-v1', the original observation\n",
    "    is an array with shape [3], so if we stack 4 observations, the processed observation\n",
    "    has shape [4, 3].\n",
    "\n",
    "    Note:\n",
    "        - To be memory efficient, the stacked observations are wrapped by :class:`LazyFrame`.\n",
    "        - The observation space must be :class:`Box` type. If one uses :class:`Dict`\n",
    "          as observation space, it should apply :class:`FlattenObservation` wrapper first.\n",
    "        - After :meth:`reset` is called, the frame buffer will be filled with the initial observation.\n",
    "          I.e. the observation returned by :meth:`reset` will consist of `num_stack` many identical frames.\n",
    "\n",
    "    Example:\n",
    "        >>> import gymnasium as gym\n",
    "        >>> from gymnasium.wrappers import FrameStack\n",
    "        >>> env = gym.make(\"CarRacing-v2\")\n",
    "        >>> env = FrameStack(env, 4)\n",
    "        >>> env.observation_space\n",
    "        Box(0, 255, (4, 96, 96, 3), uint8)\n",
    "        >>> obs, _ = env.reset()\n",
    "        >>> obs.shape\n",
    "        (4, 96, 96, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        num_stack: int,\n",
    "        lz4_compress: bool = False,\n",
    "    ):\n",
    "        \"\"\"Observation wrapper that stacks the observations in a rolling manner.\n",
    "\n",
    "        Args:\n",
    "            env (Env): The environment to apply the wrapper\n",
    "            num_stack (int): The number of frames to stack\n",
    "            lz4_compress (bool): Use lz4 to compress the frames internally\n",
    "        \"\"\"\n",
    "        gym.utils.RecordConstructorArgs.__init__(\n",
    "            self, num_stack=num_stack, lz4_compress=lz4_compress\n",
    "        )\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.num_stack = num_stack\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], num_stack, axis=0)\n",
    "        high = np.repeat(\n",
    "            self.observation_space.high[np.newaxis, ...], num_stack, axis=0\n",
    "        )\n",
    "        self.observation_space = Box(\n",
    "            low=low, high=high, dtype=self.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"Converts the wrappers current frames to lazy frames.\n",
    "\n",
    "        Args:\n",
    "            observation: Ignored\n",
    "\n",
    "        Returns:\n",
    "            :class:`LazyFrames` object for the wrapper's frame buffer,  :attr:`self.frames`\n",
    "        \"\"\"\n",
    "        assert len(self.frames) == self.num_stack, (len(self.frames), self.num_stack)\n",
    "        return LazyFrames(list(self.frames), self.lz4_compress)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Steps through the environment, appending the observation to the frame buffer.\n",
    "\n",
    "        Args:\n",
    "            action: The action to step through the environment with\n",
    "\n",
    "        Returns:\n",
    "            Stacked observations, reward, terminated, truncated, and information from the environment\n",
    "        \"\"\"\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self.observation(None), reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset the environment with kwargs.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: The kwargs for the environment reset\n",
    "\n",
    "        Returns:\n",
    "            The stacked observations\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        [self.frames.append(obs) for _ in range(self.num_stack)]\n",
    "\n",
    "        return self.observation(None), info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca77ccc",
   "metadata": {},
   "source": [
    "BUFFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65cdb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, max_size, input_shape, device='cpu'):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_ctr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.uint8)\n",
    "        self.next_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.uint8)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        if self.mem_ctr > (batch_size * 5):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        index = self.mem_ctr % self.mem_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.action_memory[index] = torch.tensor(action).detach().cpu()\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_ctr += 1\n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_ctr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b27ab",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06265025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, action_dim, hidden_dim=512, observation_shape=(4, 84, 84)):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        # Original: in_channels=1, out_channels=8, kernel_size=4, stride=2\n",
    "        self.conv1 = nn.Conv2d(in_channels=observation_shape[0], out_channels=32, kernel_size=8, stride=4)\n",
    "        # Original: in_channels=8, out_channels=16, kernel_size=4, stride=2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        # Original: in_channels=16, out_channels=32, kernel_size=3, stride=2\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Compute conv output size automatically\n",
    "        conv_output_size = self.calculate_conv_output(observation_shape)\n",
    "        print(\"conv_output_size: \", conv_output_size)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Original had 3 FC layers; now only 2 FC layers before output\n",
    "        self.fc1 = nn.Linear(conv_output_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)  # Optional: can remove this if you want exactly 2 FC\n",
    "\n",
    "        self.output = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.weights_init)\n",
    "\n",
    "    def calculate_conv_output(self, shape):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        o = F.relu(self.conv1(o))\n",
    "        o = F.relu(self.conv2(o))\n",
    "        o = F.relu(self.conv3(o))\n",
    "        return int(o.numel())\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))  # Optional if keeping 3 FC layers\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "    def save_the_model(self, filename='models/latest.pt'):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load_the_model(self, filename='models/latest.pt'):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "            print(f\"Loaded weights from {filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No weights file found at at {filename}\")\n",
    "\n",
    "\n",
    "def soft_update(target, source, tau=0.005):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "# CNN - Recognize Image\n",
    "# FC layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e628a2",
   "metadata": {},
   "source": [
    "AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff77f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import ReplayBuffer\n",
    "from model import Model, soft_update\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, env, hidden_layer, learning_rate, step_repeat, gamma):\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "        self.step_repeat = step_repeat\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        obs, info = self.env.reset()\n",
    "\n",
    "        obs = self.process_observation(obs)\n",
    "\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        print(f'Loaded model on device {self.device}')\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size=500000, input_shape=obs.shape, device=self.device)\n",
    "\n",
    "        self.model = Model(action_dim=env.action_space.n, hidden_dim=hidden_layer, observation_shape=obs.shape).to(self.device)\n",
    "        \n",
    "        self.target_model = Model(action_dim=env.action_space.n, hidden_dim=hidden_layer, observation_shape=obs.shape).to(self.device)\n",
    "\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def process_observation(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1)\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "\n",
    "        self.model.load_the_model()\n",
    "\n",
    "        obs, info = self.env.reset()\n",
    "\n",
    "        done = False\n",
    "        obs, info = self.env.reset()\n",
    "        obs = self.process_observation(obs)\n",
    "\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            if random.random() < 0.05:\n",
    "                action = self.env.action_space.sample()\n",
    "            else:\n",
    "                q_values = self.model.forward(obs.unsqueeze(0).to(self.device))[0]\n",
    "                action = torch.argmax(q_values, dim=-1).item()\n",
    "            \n",
    "            reward = 0\n",
    "\n",
    "            for i in range(self.step_repeat):\n",
    "                reward_temp = 0\n",
    "\n",
    "                next_obs, reward_temp, done, truncated, info = self.env.step(action=action)\n",
    "\n",
    "                reward += reward_temp\n",
    "\n",
    "                # frame = self.env.env.env.render() \n",
    "\n",
    "                # resized_frame = cv2.resize(frame, (500, 400))\n",
    "\n",
    "                # resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # cv2.imshow(\"Pong AI\", resized_frame)\n",
    "\n",
    "                # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                #     break\n",
    "\n",
    "                # # time.sleep(0.05) #adjust speed to control playback speed\n",
    "\n",
    "                if(done):\n",
    "                    break\n",
    "            \n",
    "            obs = self.process_observation(next_obs)\n",
    "\n",
    "            episode_reward += reward\n",
    "    \n",
    "\n",
    "    def train(self, episodes, max_episode_steps, summary_writer_suffix, batch_size, epsilon, epsilon_decay, min_epsilon):\n",
    "\n",
    "            # Save TensorBoard logs in the same directory as the script\n",
    "            script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            summary_writer_name = os.path.join(\n",
    "                script_dir, 'runs',\n",
    "                f'{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_{summary_writer_suffix}'\n",
    "            )\n",
    "            writer = SummaryWriter(summary_writer_name)\n",
    "\n",
    "            if not os.path.exists('models'):\n",
    "                os.makedirs('models')\n",
    "\n",
    "            total_steps = 0\n",
    "            reward_history, loss_history, qvalue_history = [], [], []\n",
    "\n",
    "            print(f\"TensorBoard logs → {summary_writer_name}\")\n",
    "\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                obs, info = self.env.reset()\n",
    "                obs = self.process_observation(obs)\n",
    "                episode_steps = 0\n",
    "                episode_losses, episode_qvalues, td_errors, action_counts = [], [], [], []\n",
    "\n",
    "                episode_start_time = time.time()\n",
    "\n",
    "                while not done and episode_steps < max_episode_steps:\n",
    "                    # ε-greedy action selection\n",
    "                    if random.random() < epsilon:\n",
    "                        action = self.env.action_space.sample()\n",
    "                    else:\n",
    "                        q_values = self.model.forward(obs.unsqueeze(0).to(self.device))[0]\n",
    "                        action = torch.argmax(q_values, dim=-1).item()\n",
    "                        episode_qvalues.append(q_values.mean().item())\n",
    "\n",
    "                    action_counts.append(action)\n",
    "\n",
    "                    reward = 0\n",
    "                    for _ in range(self.step_repeat):\n",
    "                        next_obs, reward_temp, done, truncated, info = self.env.step(action=action)\n",
    "                        reward += reward_temp\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "                    next_obs = self.process_observation(next_obs)\n",
    "                    self.memory.store_transition(obs, action, reward, next_obs, done)\n",
    "                    obs = next_obs\n",
    "\n",
    "                    episode_reward += reward\n",
    "                    episode_steps += 1\n",
    "                    total_steps += 1\n",
    "\n",
    "                    # Training updates\n",
    "                    if self.memory.can_sample(batch_size):\n",
    "                        observations, actions, rewards, next_observations, dones = self.memory.sample_buffer(batch_size)\n",
    "                        dones = dones.unsqueeze(1).float()\n",
    "\n",
    "                        # Q(s,a)\n",
    "                        q_values = self.model(observations)\n",
    "                        actions = actions.unsqueeze(1).long()\n",
    "                        qsa_batch = q_values.gather(1, actions)\n",
    "\n",
    "                        # Q-target(s’, a’)\n",
    "                        next_actions = torch.argmax(self.model(next_observations), dim=1, keepdim=True)\n",
    "                        next_q_values = self.target_model(next_observations).gather(1, next_actions)\n",
    "                        target_b = rewards.unsqueeze(1) + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "                        # TD error & loss\n",
    "                        td_error = (target_b.detach() - qsa_batch)\n",
    "                        loss = F.mse_loss(qsa_batch, target_b.detach())\n",
    "\n",
    "                        episode_losses.append(loss.item())\n",
    "                        td_errors.extend(td_error.abs().detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "\n",
    "                        # Optimization step\n",
    "                        self.model.zero_grad()\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                        if episode_steps % 4 == 0:\n",
    "                            soft_update(self.target_model, self.model)\n",
    "\n",
    "                        # Log per-step metrics\n",
    "                        writer.add_scalar(\"Loss/step\", loss.item(), total_steps)\n",
    "                        writer.add_scalar(\"QValue/mean\", q_values.mean().item(), total_steps)\n",
    "                        writer.add_scalar(\"QValue/max\", q_values.max().item(), total_steps)\n",
    "                        writer.add_scalar(\"TD_Error/mean\", td_error.abs().mean().item(), total_steps)\n",
    "\n",
    "                # Model checkpoint\n",
    "                self.model.save_the_model()\n",
    "\n",
    "                # Episode metrics\n",
    "                episode_time = time.time() - episode_start_time\n",
    "                avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
    "                avg_qvalue = np.mean(episode_qvalues) if episode_qvalues else 0\n",
    "                avg_td_error = np.mean(td_errors) if td_errors else 0\n",
    "                fps = episode_steps / episode_time if episode_time > 0 else 0\n",
    "                avg_reward = np.mean(reward_history[-10:]) if reward_history else 0\n",
    "\n",
    "                reward_history.append(episode_reward)\n",
    "                loss_history.append(avg_loss)\n",
    "                qvalue_history.append(avg_qvalue)\n",
    "\n",
    "                # TensorBoard logs (episode level)\n",
    "                writer.add_scalar('Score/Episode', episode_reward, episode)\n",
    "                writer.add_scalar('Score/Avg10', avg_reward, episode)\n",
    "                writer.add_scalar('Loss/Episode', avg_loss, episode)\n",
    "                writer.add_scalar('QValue/Avg', avg_qvalue, episode)\n",
    "                writer.add_scalar('TD_Error/EpisodeMean', avg_td_error, episode)\n",
    "                writer.add_scalar('Epsilon', epsilon, episode)\n",
    "                writer.add_scalar('Performance/FPS', fps, episode)\n",
    "                writer.add_scalar('Performance/EpisodeTime', episode_time, episode)\n",
    "\n",
    "                # Action distribution histogram\n",
    "                writer.add_histogram('Action/Distribution', np.array(action_counts), episode)\n",
    "\n",
    "                # Console output\n",
    "                print(f\"\\nEpisode {episode + 1}/{episodes}\")\n",
    "                print(f\"  Reward: {episode_reward:.2f}  |  Avg(10): {avg_reward:.2f}\")\n",
    "                print(f\"  Avg Loss: {avg_loss:.6f}  |  Avg Q: {avg_qvalue:.3f}  |  TD Error: {avg_td_error:.4f}\")\n",
    "                print(f\"  Steps: {episode_steps}  |  FPS: {fps:.2f}  |  Time: {episode_time:.2f}s\")\n",
    "                print(f\"  Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "                # Epsilon decay\n",
    "                if epsilon > min_epsilon:\n",
    "                    epsilon *= epsilon_decay\n",
    "\n",
    "            writer.close()\n",
    "            print(\"\\n✅ Training complete! You can now view metrics with:\")\n",
    "            print(f\"   tensorboard --logdir {os.path.join(script_dir, 'runs')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ea1a4f",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformObservation, ResizeObservation, GrayscaleObservation\n",
    "\n",
    "import ale_py\n",
    "import cv2\n",
    "\n",
    "def to_grayscale(obs):\n",
    "    return cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "episodes = 2500\n",
    "max_episode_steps = 5000\n",
    "hidden_layer = 128\n",
    "learning_rate = 0.0001\n",
    "step_repeat = 4\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "env = ResizeObservation(env, (64, 64))\n",
    "\n",
    "env = GrayscaleObservation(env,  keep_dim=True)\n",
    "\n",
    "agent = Agent(env, hidden_layer=hidden_layer,\n",
    "              learning_rate=learning_rate, step_repeat=step_repeat,\n",
    "              gamma=gamma)\n",
    "\n",
    "summary_writer_suffix = f'dqn_lr={learning_rate}_hl={hidden_layer}_bs={batch_size}'\n",
    "\n",
    "agent.train(episodes=episodes,\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            summary_writer_suffix=summary_writer_suffix,\n",
    "            batch_size=batch_size,\n",
    "            epsilon=epsilon,\n",
    "            epsilon_decay=epsilon_decay,\n",
    "            min_epsilon=min_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa50a1",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd09263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agent import Agent\n",
    "# import gymnasium as gym\n",
    "# from gymnasium.wrappers import GrayscaleObservation, ResizeObservation\n",
    "# from gymnasium.wrappers import TransformObservation, ResizeObservation\n",
    "\n",
    "# import ale_py\n",
    "# import cv2  \n",
    "\n",
    "# def to_grayscale(obs):\n",
    "#     return cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "# episodes = 10000\n",
    "# max_episode_steps = 10000\n",
    "# hidden_layer = 128\n",
    "# learning_rate = 0.0001\n",
    "# step_repeat = 4\n",
    "# gamma = 0.99\n",
    "# batch_size = 64\n",
    "# epsilon = 1\n",
    "# min_epsilon = 0.1\n",
    "# epsilon_decay = 0.995\n",
    "\n",
    "\n",
    "# env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "# env = ResizeObservation(env, (64, 64))\n",
    "\n",
    "# env = GrayscaleObservation(env,  keep_dim=True)\n",
    "\n",
    "# # env = TransformObservation(env,  f=to_grayscale)\n",
    "\n",
    "# agent = Agent(env, hidden_layer=hidden_layer,\n",
    "#               learning_rate=learning_rate, step_repeat=step_repeat,\n",
    "#               gamma=gamma)\n",
    "\n",
    "# agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
