{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c121cc49",
   "metadata": {},
   "source": [
    "# PPO for Pong (PPO-PONG.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3fe1c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports and utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# import gym\n",
    "import gymnasium as gymn\n",
    "from collections import deque, namedtuple\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import random\n",
    "from typing import Tuple\n",
    "import ale_py\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eba5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config (change as needed)\n",
    "env_id = 'ALE/Pong-v5'  # change if necessary (e.g., 'PongNoFrameskip-v4')\n",
    "seed = 0\n",
    "torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "\n",
    "# PPO hyperparameters\n",
    "cfg = {\n",
    "    'lr': 2.5e-4,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_eps': 0.2,\n",
    "    'update_epochs': 4,\n",
    "    'minibatch_size': 64,\n",
    "    'rollout_steps': 2048,  # number of env steps per update\n",
    "    'max_frames': 5_000_000, # cap training frames (adjust as needed)\n",
    "    'eval_interval': 100_000,\n",
    "    'num_eval_episodes': 10,\n",
    "}\n",
    "\n",
    "# environment preprocessing parameters\n",
    "input_shape = (4, 84, 84)  # frame stack, HxW\n",
    "frame_skip = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b3fdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple preprocessing wrappers to mimic Atari pipelines\n",
    "import cv2\n",
    "\n",
    "class PreprocessFrame:\n",
    "    def __init__(self, shape=(84,84)):\n",
    "        self.shape = shape\n",
    "    def __call__(self, frame):\n",
    "        # frame: HxWxC (uint8)\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return img.astype(np.uint8)\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.frames = deque(maxlen=k)\n",
    "    def reset(self, obs):\n",
    "        processed = obs\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(processed)\n",
    "        return np.stack(self.frames, axis=0)\n",
    "    def step(self, obs):\n",
    "        self.frames.append(obs)\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "# Helper to create env with preprocessing\n",
    "def make_env(env_id, seed=0):\n",
    "    # Using gymnasium/ALE env\n",
    "    env = gymn.make(env_id, render_mode=None)\n",
    "    env.reset(seed=seed)\n",
    "    preproc = PreprocessFrame((84,84))\n",
    "    return env, preproc\n",
    "\n",
    "# Note: for speed and correctness you might prefer standard wrappers like gym.wrappers AtariPreprocessing and FrameStack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdc2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic network with CNN backbone (compatible with DQN convs)\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_channels, n_actions, hidden_dim=512):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # convs similar to DQN-style\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # compute conv output\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, input_channels, 84, 84)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.relu(self.conv3(x))\n",
    "            conv_out_size = int(x.numel())\n",
    "        \n",
    "        self.fc = nn.Linear(conv_out_size, hidden_dim)\n",
    "        # actor & critic heads\n",
    "        self.policy = nn.Linear(hidden_dim, n_actions)\n",
    "        self.value = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expects x shape (B, C, H, W), float, normalized [0,1]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return self.policy(x), self.value(x)\n",
    "\n",
    "    def act(self, obs):\n",
    "        # obs: np array (C,H,W)\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)/255.0\n",
    "        logits, value = self.forward(obs_t)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action).item(), value.item(), probs.detach().cpu().numpy()\n",
    "\n",
    "    def get_values(self, obs_batch):\n",
    "        # obs_batch: tensor (B,C,H,W) float [0,1]\n",
    "        logits, values = self.forward(obs_batch)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        return dist, values.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8864a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, preproc, n_actions, input_channels=4, cfg=cfg):\n",
    "        self.env = env\n",
    "        self.preproc = preproc\n",
    "        self.n_actions = n_actions\n",
    "        self.cfg = cfg\n",
    "        self.net = ActorCritic(input_channels, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=cfg['lr'])\n",
    "        self.gamma = cfg['gamma']\n",
    "        self.gae_lambda = cfg['gae_lambda']\n",
    "        self.clip_eps = cfg['clip_eps']\n",
    "        self.update_epochs = cfg['update_epochs']\n",
    "        self.minibatch_size = cfg['minibatch_size']\n",
    "        self.rollout_steps = cfg['rollout_steps']\n",
    "        \n",
    "    def collect_rollout(self):\n",
    "        # collect rollout_steps transitions\n",
    "        obs = self.env.reset()[0]\n",
    "        obs = self.preproc(obs)\n",
    "        fs = FrameStack(4)\n",
    "        state = fs.reset(obs)\n",
    "        rollout = []\n",
    "        total_steps = 0\n",
    "        ep_rewards = []\n",
    "        ep_reward = 0\n",
    "        while total_steps < self.rollout_steps:\n",
    "            # run one step\n",
    "            action, logp, value, _ = self.net.act(state)\n",
    "            next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_obs_p = self.preproc(next_obs)\n",
    "            next_state = fs.step(next_obs_p)\n",
    "            rollout.append((state, action, logp, reward, value, done))\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                ep_rewards.append(ep_reward)\n",
    "                ep_reward = 0\n",
    "                obs = self.env.reset()[0]\n",
    "                obs = self.preproc(obs)\n",
    "                state = fs.reset(obs)\n",
    "        return rollout, ep_rewards\n",
    "\n",
    "    def compute_gae(self, rollout):\n",
    "        # rollout is list of (s,a,logp,r,v,done)\n",
    "        states = []\n",
    "        actions = []\n",
    "        old_logps = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        dones = []\n",
    "        for (s,a,logp,r,v,d) in rollout:\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            old_logps.append(logp)\n",
    "            rewards.append(r)\n",
    "            values.append(v)\n",
    "            dones.append(d)\n",
    "        # compute advantages\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        next_value = 0\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - float(dones[step])\n",
    "            delta = rewards[step] + self.gamma * next_value * mask - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * mask * gae\n",
    "            advantages.insert(0, gae)\n",
    "            next_value = values[step]\n",
    "            returns.insert(0, gae + values[step])\n",
    "        # convert to tensors\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=device)/255.0\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "        old_logps = torch.tensor(old_logps, dtype=torch.float32, device=device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        return states, actions, old_logps, returns, advantages\n",
    "\n",
    "    def update(self, rollout):\n",
    "        states, actions, old_logps, returns, advantages = self.compute_gae(rollout)\n",
    "        n = states.size(0)\n",
    "        for epoch in range(self.update_epochs):\n",
    "            perm = torch.randperm(n)\n",
    "            for i in range(0, n, self.minibatch_size):\n",
    "                idx = perm[i:i+self.minibatch_size]\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_logps = old_logps[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_adv = advantages[idx]\n",
    "                dist, values = self.net.get_values(batch_states)\n",
    "                new_logps = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                ratio = (new_logps - batch_old_logps).exp()\n",
    "                surr1 = ratio * batch_adv\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * batch_adv\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.mse_loss(values, batch_returns)\n",
    "                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ed0919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode/Reward\u001b[39m\u001b[38;5;124m'\u001b[39m, r, frame_count)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# evaluation logging\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m%\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_interval\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# run evaluation episodes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 104\u001b[0m, in \u001b[0;36mPPOAgent.update\u001b[1;34m(self, rollout)\u001b[0m\n\u001b[0;32m    102\u001b[0m loss \u001b[38;5;241m=\u001b[39m policy_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m value_loss \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;241m*\u001b[39m entropy\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 104\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vernon\\miniconda3\\envs\\pong\\lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/PPO_Pong')\n",
    "agent_env, preproc = make_env(env_id, seed=seed)\n",
    "# get action space\n",
    "# reset to get initial obs for determining action space\n",
    "obs0 = agent_env.reset()[0]\n",
    "# determine discrete action size\n",
    "n_actions = agent_env.action_space.n\n",
    "print('Action space:', n_actions)\n",
    "\n",
    "agent = PPOAgent(agent_env, preproc, n_actions, input_channels=4, cfg=cfg)\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "episode = 0\n",
    "all_episode_rewards = []\n",
    "\n",
    "while frame_count < cfg['max_frames']:\n",
    "    rollout, ep_rewards = agent.collect_rollout()\n",
    "    frame_count += len(rollout)\n",
    "    if len(ep_rewards)>0:\n",
    "        for r in ep_rewards:\n",
    "            all_episode_rewards.append(r)\n",
    "            writer.add_scalar('Episode/Reward', r, frame_count)\n",
    "    # update\n",
    "    agent.update(rollout)\n",
    "    # evaluation logging\n",
    "    if frame_count % cfg['eval_interval'] == 0:\n",
    "        # run evaluation episodes\n",
    "        eval_rewards = []\n",
    "        for _ in range(cfg['num_eval_episodes']):\n",
    "            obs = agent_env.reset()[0]\n",
    "            obs_p = preproc(obs)\n",
    "            fs = FrameStack(4)\n",
    "            state = fs.reset(obs_p)\n",
    "            done = False\n",
    "            ep_r = 0\n",
    "            while not done:\n",
    "                action, _, _, _ = agent.net.act(state)\n",
    "                next_obs, reward, term, trunc, info = agent_env.step(action)\n",
    "                done = term or trunc\n",
    "                next_obs_p = preproc(next_obs)\n",
    "                state = fs.step(next_obs_p)\n",
    "                ep_r += reward\n",
    "            eval_rewards.append(ep_r)\n",
    "        avg_eval = np.mean(eval_rewards)\n",
    "        writer.add_scalar('Eval/Reward', avg_eval, frame_count)\n",
    "        print(f'Frames: {frame_count} | AvgEval: {avg_eval:.2f} | Time: {time.time()-start_time:.1f}s')\n",
    "\n",
    "print('Training finished. Total frames:', frame_count)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test / Play with trained model (manual run)\n",
    "# Warning: this cell assumes you'll run after some training or load a saved model checkpoint.\n",
    "\n",
    "env, preproc = make_env(env_id)\n",
    "fs = FrameStack(4)\n",
    "obs = env.reset()[0]\n",
    "obs_p = preproc(obs)\n",
    "state = fs.reset(obs_p)\n",
    "\n",
    "for ep in range(3):\n",
    "    done = False\n",
    "    ep_r = 0\n",
    "    while not done:\n",
    "        action, _, _, _ = agent.net.act(state)\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "        done = term or trunc\n",
    "        obs_p = preproc(obs)\n",
    "        state = fs.step(obs_p)\n",
    "        ep_r += reward\n",
    "    print('Eval episode reward:', ep_r)\n",
    "\n",
    "# To render, change env creation to include render_mode='human' and call env.render() as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(agent.net.state_dict(), 'ppo_pong_net.pth')\n",
    "print('Saved ppo_pong_net.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plotting FROM logged arrays (if available)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(all_episode_rewards) > 0:\n",
    "    plt.plot(np.convolve(all_episode_rewards, np.ones(50)/50, mode='valid'))\n",
    "    plt.title('Smoothed Episode Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No episode rewards collected yet.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
